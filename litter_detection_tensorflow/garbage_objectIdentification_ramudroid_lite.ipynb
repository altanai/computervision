{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vSdG6sAIQn"
   },
   "source": [
    "# garbage_objectIdentification_ramudroid\n",
    "\n",
    "Model Training with TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaWdLA3fQDK2"
   },
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j4MGqyKQEo4",
    "outputId": "915d8deb-58c6-48c6-8ff1-872964a17d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN2N6hPEP-Ay"
   },
   "source": [
    "## Build a model for tarining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d8577c80"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "\n",
    "class Model(tf.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),\n",
    "        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n",
    "        tf.keras.layers.Dense(10, name='dense_2')\n",
    "    ])\n",
    "\n",
    "    self.model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "  # The `train` function takes a batch of input images and labels.\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
    "      tf.TensorSpec([None, 10], tf.float32),\n",
    "  ])\n",
    "  def train(self, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      prediction = self.model(x)\n",
    "      loss = self.model.loss(y, prediction)\n",
    "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "    self.model.optimizer.apply_gradients(\n",
    "        zip(gradients, self.model.trainable_variables))\n",
    "    result = {\"loss\": loss}\n",
    "    return result\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
    "  ])\n",
    "  def infer(self, x):\n",
    "    logits = self.model(x)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"output\": probabilities,\n",
    "        \"logits\": logits\n",
    "    }\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def save(self, checkpoint_path):\n",
    "    tensor_names = [weight.name for weight in self.model.weights]\n",
    "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
    "    tf.raw_ops.Save(\n",
    "        filename=checkpoint_path, tensor_names=tensor_names,\n",
    "        data=tensors_to_save, name='save')\n",
    "    return {\n",
    "        \"checkpoint_path\": checkpoint_path\n",
    "    }\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def restore(self, checkpoint_path):\n",
    "    restored_tensors = {}\n",
    "    for var in self.model.weights:\n",
    "      restored = tf.raw_ops.Restore(\n",
    "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
    "          name='restore')\n",
    "      var.assign(restored)\n",
    "      restored_tensors[var.name] = restored\n",
    "    return restored_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86tBMjoBnp6d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a813419961ef"
   },
   "source": [
    "## Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "315b8b4dfc16",
    "outputId": "1585670f-b6bc-4b62-ad36-ac257b46c1a1"
   },
   "outputs": [],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# builder = tfds.ImageFolder('litter_sample_dataset')\n",
    "# print(builder.info)  # num examples, labels... are automatically calculated\n",
    "# ds = builder.as_dataset(split='train', shuffle_files=True)\n",
    "# tfds.show_examples(ds, builder.info)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Oc9ZzdvgGL25"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# dataset_url = \"litter_sample_dataset.tar.xz\"\n",
    "# data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "#                                    fname='litter_dataset',\n",
    "#                                    untar=True)\n",
    "# data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# list_ds = tf.data.Dataset.list_files(str(data_dir)+ str('*/*.jpg'))\n",
    "# (train_images, train_labels), (test_images, test_labels) = list_ds.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UYuYsHQ0FRnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3644\n",
      "[PosixPath('litter_sample_dataset/paper/paper283.jpg'), PosixPath('litter_sample_dataset/paper/paper297.jpg'), PosixPath('litter_sample_dataset/paper/paper526.jpg'), PosixPath('litter_sample_dataset/paper/paper240.jpg'), PosixPath('litter_sample_dataset/paper/paper254.jpg'), PosixPath('litter_sample_dataset/paper/paper532.jpg'), PosixPath('litter_sample_dataset/paper/paper268.jpg'), PosixPath('litter_sample_dataset/paper/paper491.jpg'), PosixPath('litter_sample_dataset/paper/paper64.jpg'), PosixPath('litter_sample_dataset/paper/paper70.jpg'), PosixPath('litter_sample_dataset/paper/paper485.jpg'), PosixPath('litter_sample_dataset/paper/paper58.jpg'), PosixPath('litter_sample_dataset/paper/paper452.jpg'), PosixPath('litter_sample_dataset/paper/paper334.jpg'), PosixPath('litter_sample_dataset/paper/paper320.jpg'), PosixPath('litter_sample_dataset/paper/paper446.jpg'), PosixPath('litter_sample_dataset/paper/paper308.jpg'), PosixPath('litter_sample_dataset/paper/paper136.jpg'), PosixPath('litter_sample_dataset/paper/paper122.jpg'), PosixPath('litter_sample_dataset/paper/paper123.jpg'), PosixPath('litter_sample_dataset/paper/paper137.jpg'), PosixPath('litter_sample_dataset/paper/paper309.jpg'), PosixPath('litter_sample_dataset/paper/paper321.jpg'), PosixPath('litter_sample_dataset/paper/paper447.jpg'), PosixPath('litter_sample_dataset/paper/paper453.jpg'), PosixPath('litter_sample_dataset/paper/paper335.jpg'), PosixPath('litter_sample_dataset/paper/paper59.jpg'), PosixPath('litter_sample_dataset/paper/paper484.jpg'), PosixPath('litter_sample_dataset/paper/paper71.jpg'), PosixPath('litter_sample_dataset/paper/paper65.jpg'), PosixPath('litter_sample_dataset/paper/paper490.jpg'), PosixPath('litter_sample_dataset/paper/paper269.jpg'), PosixPath('litter_sample_dataset/paper/paper255.jpg'), PosixPath('litter_sample_dataset/paper/paper533.jpg'), PosixPath('litter_sample_dataset/paper/paper527.jpg'), PosixPath('litter_sample_dataset/paper/paper241.jpg'), PosixPath('litter_sample_dataset/paper/paper296.jpg'), PosixPath('litter_sample_dataset/paper/paper282.jpg'), PosixPath('litter_sample_dataset/paper/paper294.jpg'), PosixPath('litter_sample_dataset/paper/paper280.jpg'), PosixPath('litter_sample_dataset/paper/paper531.jpg'), PosixPath('litter_sample_dataset/paper/paper257.jpg'), PosixPath('litter_sample_dataset/paper/paper243.jpg'), PosixPath('litter_sample_dataset/paper/paper525.jpg'), PosixPath('litter_sample_dataset/paper/paper519.jpg'), PosixPath('litter_sample_dataset/paper/paper73.jpg'), PosixPath('litter_sample_dataset/paper/paper486.jpg'), PosixPath('litter_sample_dataset/paper/paper492.jpg'), PosixPath('litter_sample_dataset/paper/paper67.jpg'), PosixPath('litter_sample_dataset/paper/paper445.jpg'), PosixPath('litter_sample_dataset/paper/paper323.jpg'), PosixPath('litter_sample_dataset/paper/paper337.jpg'), PosixPath('litter_sample_dataset/paper/paper451.jpg'), PosixPath('litter_sample_dataset/paper/paper479.jpg'), PosixPath('litter_sample_dataset/paper/paper98.jpg'), PosixPath('litter_sample_dataset/paper/paper121.jpg'), PosixPath('litter_sample_dataset/paper/paper135.jpg'), PosixPath('litter_sample_dataset/paper/paper109.jpg'), PosixPath('litter_sample_dataset/paper/paper108.jpg'), PosixPath('litter_sample_dataset/paper/paper134.jpg'), PosixPath('litter_sample_dataset/paper/paper120.jpg'), PosixPath('litter_sample_dataset/paper/paper99.jpg'), PosixPath('litter_sample_dataset/paper/paper478.jpg'), PosixPath('litter_sample_dataset/paper/paper336.jpg'), PosixPath('litter_sample_dataset/paper/paper450.jpg'), PosixPath('litter_sample_dataset/paper/paper444.jpg'), PosixPath('litter_sample_dataset/paper/paper322.jpg'), PosixPath('litter_sample_dataset/paper/paper66.jpg'), PosixPath('litter_sample_dataset/paper/paper493.jpg'), PosixPath('litter_sample_dataset/paper/paper487.jpg'), PosixPath('litter_sample_dataset/paper/paper72.jpg'), PosixPath('litter_sample_dataset/paper/paper518.jpg'), PosixPath('litter_sample_dataset/paper/paper242.jpg'), PosixPath('litter_sample_dataset/paper/paper524.jpg'), PosixPath('litter_sample_dataset/paper/paper530.jpg'), PosixPath('litter_sample_dataset/paper/paper256.jpg'), PosixPath('litter_sample_dataset/paper/paper281.jpg'), PosixPath('litter_sample_dataset/paper/paper295.jpg'), PosixPath('litter_sample_dataset/paper/paper291.jpg'), PosixPath('litter_sample_dataset/paper/paper285.jpg'), PosixPath('litter_sample_dataset/paper/paper508.jpg'), PosixPath('litter_sample_dataset/paper/paper252.jpg'), PosixPath('litter_sample_dataset/paper/paper534.jpg'), PosixPath('litter_sample_dataset/paper/paper520.jpg'), PosixPath('litter_sample_dataset/paper/paper246.jpg'), PosixPath('litter_sample_dataset/paper/paper76.jpg'), PosixPath('litter_sample_dataset/paper/paper483.jpg'), PosixPath('litter_sample_dataset/paper/paper497.jpg'), PosixPath('litter_sample_dataset/paper/paper62.jpg'), PosixPath('litter_sample_dataset/paper/paper89.jpg'), PosixPath('litter_sample_dataset/paper/paper468.jpg'), PosixPath('litter_sample_dataset/paper/paper326.jpg'), PosixPath('litter_sample_dataset/paper/paper440.jpg'), PosixPath('litter_sample_dataset/paper/paper454.jpg'), PosixPath('litter_sample_dataset/paper/paper332.jpg'), PosixPath('litter_sample_dataset/paper/paper118.jpg'), PosixPath('litter_sample_dataset/paper/paper124.jpg'), PosixPath('litter_sample_dataset/paper/paper130.jpg'), PosixPath('litter_sample_dataset/paper/paper131.jpg'), PosixPath('litter_sample_dataset/paper/paper125.jpg'), PosixPath('litter_sample_dataset/paper/paper119.jpg'), PosixPath('litter_sample_dataset/paper/paper455.jpg'), PosixPath('litter_sample_dataset/paper/paper333.jpg'), PosixPath('litter_sample_dataset/paper/paper327.jpg'), PosixPath('litter_sample_dataset/paper/paper441.jpg'), PosixPath('litter_sample_dataset/paper/paper469.jpg'), PosixPath('litter_sample_dataset/paper/paper88.jpg'), PosixPath('litter_sample_dataset/paper/paper63.jpg'), PosixPath('litter_sample_dataset/paper/paper496.jpg'), PosixPath('litter_sample_dataset/paper/paper482.jpg'), PosixPath('litter_sample_dataset/paper/paper77.jpg'), PosixPath('litter_sample_dataset/paper/paper521.jpg'), PosixPath('litter_sample_dataset/paper/paper247.jpg'), PosixPath('litter_sample_dataset/paper/paper253.jpg'), PosixPath('litter_sample_dataset/paper/paper535.jpg'), PosixPath('litter_sample_dataset/paper/paper509.jpg'), PosixPath('litter_sample_dataset/paper/paper284.jpg'), PosixPath('litter_sample_dataset/paper/paper290.jpg'), PosixPath('litter_sample_dataset/paper/paper286.jpg'), PosixPath('litter_sample_dataset/paper/paper292.jpg'), PosixPath('litter_sample_dataset/paper/paper279.jpg'), PosixPath('litter_sample_dataset/paper/paper245.jpg'), PosixPath('litter_sample_dataset/paper/paper523.jpg'), PosixPath('litter_sample_dataset/paper/paper537.jpg'), PosixPath('litter_sample_dataset/paper/paper251.jpg'), PosixPath('litter_sample_dataset/paper/paper49.jpg'), PosixPath('litter_sample_dataset/paper/paper494.jpg'), PosixPath('litter_sample_dataset/paper/paper61.jpg'), PosixPath('litter_sample_dataset/paper/paper75.jpg'), PosixPath('litter_sample_dataset/paper/paper480.jpg'), PosixPath('litter_sample_dataset/paper/paper319.jpg'), PosixPath('litter_sample_dataset/paper/paper331.jpg'), PosixPath('litter_sample_dataset/paper/paper457.jpg'), PosixPath('litter_sample_dataset/paper/paper443.jpg'), PosixPath('litter_sample_dataset/paper/paper325.jpg'), PosixPath('litter_sample_dataset/paper/paper133.jpg'), PosixPath('litter_sample_dataset/paper/paper127.jpg'), PosixPath('litter_sample_dataset/paper/paper126.jpg'), PosixPath('litter_sample_dataset/paper/paper132.jpg'), PosixPath('litter_sample_dataset/paper/paper442.jpg'), PosixPath('litter_sample_dataset/paper/paper324.jpg'), PosixPath('litter_sample_dataset/paper/paper330.jpg'), PosixPath('litter_sample_dataset/paper/paper456.jpg'), PosixPath('litter_sample_dataset/paper/paper318.jpg'), PosixPath('litter_sample_dataset/paper/paper481.jpg'), PosixPath('litter_sample_dataset/paper/paper74.jpg'), PosixPath('litter_sample_dataset/paper/paper60.jpg'), PosixPath('litter_sample_dataset/paper/paper495.jpg'), PosixPath('litter_sample_dataset/paper/paper48.jpg'), PosixPath('litter_sample_dataset/paper/paper536.jpg'), PosixPath('litter_sample_dataset/paper/paper250.jpg'), PosixPath('litter_sample_dataset/paper/paper244.jpg'), PosixPath('litter_sample_dataset/paper/paper522.jpg'), PosixPath('litter_sample_dataset/paper/paper278.jpg'), PosixPath('litter_sample_dataset/paper/paper293.jpg'), PosixPath('litter_sample_dataset/paper/paper287.jpg'), PosixPath('litter_sample_dataset/paper/paper586.jpg'), PosixPath('litter_sample_dataset/paper/paper592.jpg'), PosixPath('litter_sample_dataset/paper/paper223.jpg'), PosixPath('litter_sample_dataset/paper/paper545.jpg'), PosixPath('litter_sample_dataset/paper/paper551.jpg'), PosixPath('litter_sample_dataset/paper/paper237.jpg'), PosixPath('litter_sample_dataset/paper/paper579.jpg'), PosixPath('litter_sample_dataset/paper/paper394.jpg'), PosixPath('litter_sample_dataset/paper/paper13.jpg'), PosixPath('litter_sample_dataset/paper/paper380.jpg'), PosixPath('litter_sample_dataset/paper/paper357.jpg'), PosixPath('litter_sample_dataset/paper/paper431.jpg'), PosixPath('litter_sample_dataset/paper/paper425.jpg'), PosixPath('litter_sample_dataset/paper/paper343.jpg'), PosixPath('litter_sample_dataset/paper/paper419.jpg'), PosixPath('litter_sample_dataset/paper/paper196.jpg'), PosixPath('litter_sample_dataset/paper/paper182.jpg'), PosixPath('litter_sample_dataset/paper/paper155.jpg'), PosixPath('litter_sample_dataset/paper/paper141.jpg'), PosixPath('litter_sample_dataset/paper/paper4.jpg'), PosixPath('litter_sample_dataset/paper/paper169.jpg'), PosixPath('litter_sample_dataset/paper/paper168.jpg'), PosixPath('litter_sample_dataset/paper/paper140.jpg'), PosixPath('litter_sample_dataset/paper/paper5.jpg'), PosixPath('litter_sample_dataset/paper/paper154.jpg'), PosixPath('litter_sample_dataset/paper/paper183.jpg'), PosixPath('litter_sample_dataset/paper/paper197.jpg'), PosixPath('litter_sample_dataset/paper/paper418.jpg'), PosixPath('litter_sample_dataset/paper/paper424.jpg'), PosixPath('litter_sample_dataset/paper/paper342.jpg'), PosixPath('litter_sample_dataset/paper/paper356.jpg'), PosixPath('litter_sample_dataset/paper/paper430.jpg'), PosixPath('litter_sample_dataset/paper/paper12.jpg'), PosixPath('litter_sample_dataset/paper/paper381.jpg'), PosixPath('litter_sample_dataset/paper/paper395.jpg'), PosixPath('litter_sample_dataset/paper/paper578.jpg'), PosixPath('litter_sample_dataset/paper/paper550.jpg'), PosixPath('litter_sample_dataset/paper/paper236.jpg'), PosixPath('litter_sample_dataset/paper/paper222.jpg'), PosixPath('litter_sample_dataset/paper/paper544.jpg'), PosixPath('litter_sample_dataset/paper/paper593.jpg'), PosixPath('litter_sample_dataset/paper/paper587.jpg'), PosixPath('litter_sample_dataset/paper/paper591.jpg'), PosixPath('litter_sample_dataset/paper/paper585.jpg'), PosixPath('litter_sample_dataset/paper/paper234.jpg'), PosixPath('litter_sample_dataset/paper/paper552.jpg'), PosixPath('litter_sample_dataset/paper/paper546.jpg'), PosixPath('litter_sample_dataset/paper/paper220.jpg'), PosixPath('litter_sample_dataset/paper/paper208.jpg'), PosixPath('litter_sample_dataset/paper/paper383.jpg'), PosixPath('litter_sample_dataset/paper/paper10.jpg'), PosixPath('litter_sample_dataset/paper/paper397.jpg'), PosixPath('litter_sample_dataset/paper/paper38.jpg'), PosixPath('litter_sample_dataset/paper/paper340.jpg'), PosixPath('litter_sample_dataset/paper/paper426.jpg'), PosixPath('litter_sample_dataset/paper/paper432.jpg'), PosixPath('litter_sample_dataset/paper/paper354.jpg'), PosixPath('litter_sample_dataset/paper/paper368.jpg'), PosixPath('litter_sample_dataset/paper/paper181.jpg'), PosixPath('litter_sample_dataset/paper/paper195.jpg'), PosixPath('litter_sample_dataset/paper/paper7.jpg'), PosixPath('litter_sample_dataset/paper/paper142.jpg'), PosixPath('litter_sample_dataset/paper/paper156.jpg'), PosixPath('litter_sample_dataset/paper/paper157.jpg'), PosixPath('litter_sample_dataset/paper/paper6.jpg'), PosixPath('litter_sample_dataset/paper/paper143.jpg'), PosixPath('litter_sample_dataset/paper/paper194.jpg'), PosixPath('litter_sample_dataset/paper/paper180.jpg'), PosixPath('litter_sample_dataset/paper/paper369.jpg'), PosixPath('litter_sample_dataset/paper/paper433.jpg'), PosixPath('litter_sample_dataset/paper/paper355.jpg'), PosixPath('litter_sample_dataset/paper/paper341.jpg'), PosixPath('litter_sample_dataset/paper/paper427.jpg'), PosixPath('litter_sample_dataset/paper/paper39.jpg'), PosixPath('litter_sample_dataset/paper/paper396.jpg'), PosixPath('litter_sample_dataset/paper/paper382.jpg'), PosixPath('litter_sample_dataset/paper/paper11.jpg'), PosixPath('litter_sample_dataset/paper/paper209.jpg'), PosixPath('litter_sample_dataset/paper/paper547.jpg'), PosixPath('litter_sample_dataset/paper/paper221.jpg'), PosixPath('litter_sample_dataset/paper/paper235.jpg'), PosixPath('litter_sample_dataset/paper/paper553.jpg'), PosixPath('litter_sample_dataset/paper/paper584.jpg'), PosixPath('litter_sample_dataset/paper/paper590.jpg'), PosixPath('litter_sample_dataset/paper/paper594.jpg'), PosixPath('litter_sample_dataset/paper/paper580.jpg'), PosixPath('litter_sample_dataset/paper/paper219.jpg'), PosixPath('litter_sample_dataset/paper/paper557.jpg'), PosixPath('litter_sample_dataset/paper/paper231.jpg'), PosixPath('litter_sample_dataset/paper/paper225.jpg'), PosixPath('litter_sample_dataset/paper/paper543.jpg'), PosixPath('litter_sample_dataset/paper/paper29.jpg'), PosixPath('litter_sample_dataset/paper/paper15.jpg'), PosixPath('litter_sample_dataset/paper/paper386.jpg'), PosixPath('litter_sample_dataset/paper/paper392.jpg'), PosixPath('litter_sample_dataset/paper/paper379.jpg'), PosixPath('litter_sample_dataset/paper/paper423.jpg'), PosixPath('litter_sample_dataset/paper/paper345.jpg'), PosixPath('litter_sample_dataset/paper/paper351.jpg'), PosixPath('litter_sample_dataset/paper/paper437.jpg'), PosixPath('litter_sample_dataset/paper/paper184.jpg'), PosixPath('litter_sample_dataset/paper/paper190.jpg'), PosixPath('litter_sample_dataset/paper/paper147.jpg'), PosixPath('litter_sample_dataset/paper/paper2.jpg'), PosixPath('litter_sample_dataset/paper/paper153.jpg'), PosixPath('litter_sample_dataset/paper/paper152.jpg'), PosixPath('litter_sample_dataset/paper/paper146.jpg'), PosixPath('litter_sample_dataset/paper/paper3.jpg'), PosixPath('litter_sample_dataset/paper/paper191.jpg'), PosixPath('litter_sample_dataset/paper/paper185.jpg'), PosixPath('litter_sample_dataset/paper/paper350.jpg'), PosixPath('litter_sample_dataset/paper/paper436.jpg'), PosixPath('litter_sample_dataset/paper/paper422.jpg'), PosixPath('litter_sample_dataset/paper/paper344.jpg'), PosixPath('litter_sample_dataset/paper/paper378.jpg'), PosixPath('litter_sample_dataset/paper/paper393.jpg'), PosixPath('litter_sample_dataset/paper/paper14.jpg'), PosixPath('litter_sample_dataset/paper/paper387.jpg'), PosixPath('litter_sample_dataset/paper/paper28.jpg'), PosixPath('litter_sample_dataset/paper/paper224.jpg'), PosixPath('litter_sample_dataset/paper/paper542.jpg'), PosixPath('litter_sample_dataset/paper/paper556.jpg'), PosixPath('litter_sample_dataset/paper/paper230.jpg'), PosixPath('litter_sample_dataset/paper/paper218.jpg'), PosixPath('litter_sample_dataset/paper/paper581.jpg'), PosixPath('litter_sample_dataset/paper/paper583.jpg'), PosixPath('litter_sample_dataset/paper/paper568.jpg'), PosixPath('litter_sample_dataset/paper/paper540.jpg'), PosixPath('litter_sample_dataset/paper/paper226.jpg'), PosixPath('litter_sample_dataset/paper/paper232.jpg'), PosixPath('litter_sample_dataset/paper/paper554.jpg'), PosixPath('litter_sample_dataset/paper/paper391.jpg'), PosixPath('litter_sample_dataset/paper/paper385.jpg'), PosixPath('litter_sample_dataset/paper/paper16.jpg'), PosixPath('litter_sample_dataset/paper/paper408.jpg'), PosixPath('litter_sample_dataset/paper/paper434.jpg'), PosixPath('litter_sample_dataset/paper/paper352.jpg'), PosixPath('litter_sample_dataset/paper/paper346.jpg'), PosixPath('litter_sample_dataset/paper/paper420.jpg'), PosixPath('litter_sample_dataset/paper/paper193.jpg'), PosixPath('litter_sample_dataset/paper/paper187.jpg'), PosixPath('litter_sample_dataset/paper/paper178.jpg'), PosixPath('litter_sample_dataset/paper/paper150.jpg'), PosixPath('litter_sample_dataset/paper/paper1.jpg'), PosixPath('litter_sample_dataset/paper/paper144.jpg'), PosixPath('litter_sample_dataset/paper/paper145.jpg'), PosixPath('litter_sample_dataset/paper/paper151.jpg'), PosixPath('litter_sample_dataset/paper/paper179.jpg'), PosixPath('litter_sample_dataset/paper/paper186.jpg'), PosixPath('litter_sample_dataset/paper/paper192.jpg'), PosixPath('litter_sample_dataset/paper/paper347.jpg'), PosixPath('litter_sample_dataset/paper/paper421.jpg'), PosixPath('litter_sample_dataset/paper/paper435.jpg'), PosixPath('litter_sample_dataset/paper/paper353.jpg'), PosixPath('litter_sample_dataset/paper/paper409.jpg'), PosixPath('litter_sample_dataset/paper/paper384.jpg'), PosixPath('litter_sample_dataset/paper/paper17.jpg'), PosixPath('litter_sample_dataset/paper/paper390.jpg'), PosixPath('litter_sample_dataset/paper/paper233.jpg'), PosixPath('litter_sample_dataset/paper/paper555.jpg'), PosixPath('litter_sample_dataset/paper/paper541.jpg'), PosixPath('litter_sample_dataset/paper/paper227.jpg'), PosixPath('litter_sample_dataset/paper/paper569.jpg'), PosixPath('litter_sample_dataset/paper/paper582.jpg'), PosixPath('litter_sample_dataset/paper/paper202.jpg'), PosixPath('litter_sample_dataset/paper/paper564.jpg'), PosixPath('litter_sample_dataset/paper/paper570.jpg'), PosixPath('litter_sample_dataset/paper/paper216.jpg'), PosixPath('litter_sample_dataset/paper/paper558.jpg'), PosixPath('litter_sample_dataset/paper/paper26.jpg'), PosixPath('litter_sample_dataset/paper/paper32.jpg'), PosixPath('litter_sample_dataset/paper/paper389.jpg'), PosixPath('litter_sample_dataset/paper/paper376.jpg'), PosixPath('litter_sample_dataset/paper/paper410.jpg'), PosixPath('litter_sample_dataset/paper/paper404.jpg'), PosixPath('litter_sample_dataset/paper/paper362.jpg'), PosixPath('litter_sample_dataset/paper/paper438.jpg'), PosixPath('litter_sample_dataset/paper/paper174.jpg'), PosixPath('litter_sample_dataset/paper/paper160.jpg'), PosixPath('litter_sample_dataset/paper/paper148.jpg'), PosixPath('litter_sample_dataset/paper/paper149.jpg'), PosixPath('litter_sample_dataset/paper/paper161.jpg'), PosixPath('litter_sample_dataset/paper/paper175.jpg'), PosixPath('litter_sample_dataset/paper/paper439.jpg'), PosixPath('litter_sample_dataset/paper/paper405.jpg'), PosixPath('litter_sample_dataset/paper/paper363.jpg'), PosixPath('litter_sample_dataset/paper/paper377.jpg'), PosixPath('litter_sample_dataset/paper/paper411.jpg'), PosixPath('litter_sample_dataset/paper/paper388.jpg'), PosixPath('litter_sample_dataset/paper/paper33.jpg'), PosixPath('litter_sample_dataset/paper/paper27.jpg'), PosixPath('litter_sample_dataset/paper/paper559.jpg'), PosixPath('litter_sample_dataset/paper/paper571.jpg'), PosixPath('litter_sample_dataset/paper/paper217.jpg'), PosixPath('litter_sample_dataset/paper/paper203.jpg'), PosixPath('litter_sample_dataset/paper/paper565.jpg'), PosixPath('litter_sample_dataset/paper/paper215.jpg'), PosixPath('litter_sample_dataset/paper/paper573.jpg'), PosixPath('litter_sample_dataset/paper/paper567.jpg'), PosixPath('litter_sample_dataset/paper/paper201.jpg'), PosixPath('litter_sample_dataset/paper/paper229.jpg'), PosixPath('litter_sample_dataset/paper/paper31.jpg'), PosixPath('litter_sample_dataset/paper/paper25.jpg'), PosixPath('litter_sample_dataset/paper/paper19.jpg'), PosixPath('litter_sample_dataset/paper/paper361.jpg'), PosixPath('litter_sample_dataset/paper/paper407.jpg'), PosixPath('litter_sample_dataset/paper/paper413.jpg'), PosixPath('litter_sample_dataset/paper/paper375.jpg'), PosixPath('litter_sample_dataset/paper/paper349.jpg'), PosixPath('litter_sample_dataset/paper/paper188.jpg'), PosixPath('litter_sample_dataset/paper/paper163.jpg'), PosixPath('litter_sample_dataset/paper/paper177.jpg'), PosixPath('litter_sample_dataset/paper/paper176.jpg'), PosixPath('litter_sample_dataset/paper/paper162.jpg'), PosixPath('litter_sample_dataset/paper/paper189.jpg'), PosixPath('litter_sample_dataset/paper/paper348.jpg'), PosixPath('litter_sample_dataset/paper/paper412.jpg'), PosixPath('litter_sample_dataset/paper/paper374.jpg'), PosixPath('litter_sample_dataset/paper/paper360.jpg'), PosixPath('litter_sample_dataset/paper/paper406.jpg'), PosixPath('litter_sample_dataset/paper/paper18.jpg'), PosixPath('litter_sample_dataset/paper/paper24.jpg'), PosixPath('litter_sample_dataset/paper/paper30.jpg'), PosixPath('litter_sample_dataset/paper/paper228.jpg'), PosixPath('litter_sample_dataset/paper/paper566.jpg'), PosixPath('litter_sample_dataset/paper/paper200.jpg'), PosixPath('litter_sample_dataset/paper/paper214.jpg'), PosixPath('litter_sample_dataset/paper/paper572.jpg'), PosixPath('litter_sample_dataset/paper/paper589.jpg'), PosixPath('litter_sample_dataset/paper/paper238.jpg'), PosixPath('litter_sample_dataset/paper/paper576.jpg'), PosixPath('litter_sample_dataset/paper/paper210.jpg'), PosixPath('litter_sample_dataset/paper/paper204.jpg'), PosixPath('litter_sample_dataset/paper/paper562.jpg'), PosixPath('litter_sample_dataset/paper/paper34.jpg'), PosixPath('litter_sample_dataset/paper/paper20.jpg'), PosixPath('litter_sample_dataset/paper/paper358.jpg'), PosixPath('litter_sample_dataset/paper/paper402.jpg'), PosixPath('litter_sample_dataset/paper/paper364.jpg'), PosixPath('litter_sample_dataset/paper/paper370.jpg'), PosixPath('litter_sample_dataset/paper/paper416.jpg'), PosixPath('litter_sample_dataset/paper/paper199.jpg'), PosixPath('litter_sample_dataset/paper/paper166.jpg'), PosixPath('litter_sample_dataset/paper/paper172.jpg'), PosixPath('litter_sample_dataset/paper/paper173.jpg'), PosixPath('litter_sample_dataset/paper/paper167.jpg'), PosixPath('litter_sample_dataset/paper/paper198.jpg'), PosixPath('litter_sample_dataset/paper/paper371.jpg'), PosixPath('litter_sample_dataset/paper/paper417.jpg'), PosixPath('litter_sample_dataset/paper/paper403.jpg'), PosixPath('litter_sample_dataset/paper/paper365.jpg'), PosixPath('litter_sample_dataset/paper/paper359.jpg'), PosixPath('litter_sample_dataset/paper/paper21.jpg'), PosixPath('litter_sample_dataset/paper/paper35.jpg'), PosixPath('litter_sample_dataset/paper/paper205.jpg'), PosixPath('litter_sample_dataset/paper/paper563.jpg'), PosixPath('litter_sample_dataset/paper/paper577.jpg'), PosixPath('litter_sample_dataset/paper/paper211.jpg'), PosixPath('litter_sample_dataset/paper/paper239.jpg'), PosixPath('litter_sample_dataset/paper/paper588.jpg'), PosixPath('litter_sample_dataset/paper/paper549.jpg'), PosixPath('litter_sample_dataset/paper/paper561.jpg'), PosixPath('litter_sample_dataset/paper/paper207.jpg'), PosixPath('litter_sample_dataset/paper/paper213.jpg'), PosixPath('litter_sample_dataset/paper/paper575.jpg'), PosixPath('litter_sample_dataset/paper/paper398.jpg'), PosixPath('litter_sample_dataset/paper/paper23.jpg'), PosixPath('litter_sample_dataset/paper/paper37.jpg'), PosixPath('litter_sample_dataset/paper/paper429.jpg'), PosixPath('litter_sample_dataset/paper/paper415.jpg'), PosixPath('litter_sample_dataset/paper/paper373.jpg'), PosixPath('litter_sample_dataset/paper/paper367.jpg'), PosixPath('litter_sample_dataset/paper/paper401.jpg'), PosixPath('litter_sample_dataset/paper/paper8.jpg'), PosixPath('litter_sample_dataset/paper/paper159.jpg'), PosixPath('litter_sample_dataset/paper/paper171.jpg'), PosixPath('litter_sample_dataset/paper/paper165.jpg'), PosixPath('litter_sample_dataset/paper/paper164.jpg'), PosixPath('litter_sample_dataset/paper/paper170.jpg'), PosixPath('litter_sample_dataset/paper/paper158.jpg'), PosixPath('litter_sample_dataset/paper/paper9.jpg'), PosixPath('litter_sample_dataset/paper/paper366.jpg'), PosixPath('litter_sample_dataset/paper/paper400.jpg'), PosixPath('litter_sample_dataset/paper/paper414.jpg'), PosixPath('litter_sample_dataset/paper/paper372.jpg'), PosixPath('litter_sample_dataset/paper/paper428.jpg'), PosixPath('litter_sample_dataset/paper/paper36.jpg'), PosixPath('litter_sample_dataset/paper/paper22.jpg'), PosixPath('litter_sample_dataset/paper/paper399.jpg'), PosixPath('litter_sample_dataset/paper/paper212.jpg'), PosixPath('litter_sample_dataset/paper/paper574.jpg'), PosixPath('litter_sample_dataset/paper/paper560.jpg'), PosixPath('litter_sample_dataset/paper/paper206.jpg'), PosixPath('litter_sample_dataset/paper/paper548.jpg'), PosixPath('litter_sample_dataset/paper/paper507.jpg'), PosixPath('litter_sample_dataset/paper/paper261.jpg'), PosixPath('litter_sample_dataset/paper/paper275.jpg'), PosixPath('litter_sample_dataset/paper/paper513.jpg'), PosixPath('litter_sample_dataset/paper/paper249.jpg'), PosixPath('litter_sample_dataset/paper/paper45.jpg'), PosixPath('litter_sample_dataset/paper/paper51.jpg'), PosixPath('litter_sample_dataset/paper/paper79.jpg'), PosixPath('litter_sample_dataset/paper/paper498.jpg'), PosixPath('litter_sample_dataset/paper/paper473.jpg'), PosixPath('litter_sample_dataset/paper/paper86.jpg'), PosixPath('litter_sample_dataset/paper/paper315.jpg'), PosixPath('litter_sample_dataset/paper/paper301.jpg'), PosixPath('litter_sample_dataset/paper/paper92.jpg'), PosixPath('litter_sample_dataset/paper/paper467.jpg'), PosixPath('litter_sample_dataset/paper/paper329.jpg'), PosixPath('litter_sample_dataset/paper/paper117.jpg'), PosixPath('litter_sample_dataset/paper/paper103.jpg'), PosixPath('litter_sample_dataset/paper/paper102.jpg'), PosixPath('litter_sample_dataset/paper/paper116.jpg'), PosixPath('litter_sample_dataset/paper/paper328.jpg'), PosixPath('litter_sample_dataset/paper/paper300.jpg'), PosixPath('litter_sample_dataset/paper/paper466.jpg'), PosixPath('litter_sample_dataset/paper/paper93.jpg'), PosixPath('litter_sample_dataset/paper/paper87.jpg'), PosixPath('litter_sample_dataset/paper/paper472.jpg'), PosixPath('litter_sample_dataset/paper/paper314.jpg'), PosixPath('litter_sample_dataset/paper/paper499.jpg'), PosixPath('litter_sample_dataset/paper/paper78.jpg'), PosixPath('litter_sample_dataset/paper/paper50.jpg'), PosixPath('litter_sample_dataset/paper/paper44.jpg'), PosixPath('litter_sample_dataset/paper/paper248.jpg'), PosixPath('litter_sample_dataset/paper/paper274.jpg'), PosixPath('litter_sample_dataset/paper/paper512.jpg'), PosixPath('litter_sample_dataset/paper/paper506.jpg'), PosixPath('litter_sample_dataset/paper/paper260.jpg'), PosixPath('litter_sample_dataset/paper/paper289.jpg'), PosixPath('litter_sample_dataset/paper/paper510.jpg'), PosixPath('litter_sample_dataset/paper/paper276.jpg'), PosixPath('litter_sample_dataset/paper/paper262.jpg'), PosixPath('litter_sample_dataset/paper/paper504.jpg'), PosixPath('litter_sample_dataset/paper/paper538.jpg'), PosixPath('litter_sample_dataset/paper/paper52.jpg'), PosixPath('litter_sample_dataset/paper/paper46.jpg'), PosixPath('litter_sample_dataset/paper/paper91.jpg'), PosixPath('litter_sample_dataset/paper/paper464.jpg'), PosixPath('litter_sample_dataset/paper/paper302.jpg'), PosixPath('litter_sample_dataset/paper/paper316.jpg'), PosixPath('litter_sample_dataset/paper/paper470.jpg'), PosixPath('litter_sample_dataset/paper/paper85.jpg'), PosixPath('litter_sample_dataset/paper/paper458.jpg'), PosixPath('litter_sample_dataset/paper/paper100.jpg'), PosixPath('litter_sample_dataset/paper/paper114.jpg'), PosixPath('litter_sample_dataset/paper/paper128.jpg'), PosixPath('litter_sample_dataset/paper/paper129.jpg'), PosixPath('litter_sample_dataset/paper/paper115.jpg'), PosixPath('litter_sample_dataset/paper/paper101.jpg'), PosixPath('litter_sample_dataset/paper/paper459.jpg'), PosixPath('litter_sample_dataset/paper/paper317.jpg'), PosixPath('litter_sample_dataset/paper/paper84.jpg'), PosixPath('litter_sample_dataset/paper/paper471.jpg'), PosixPath('litter_sample_dataset/paper/paper465.jpg'), PosixPath('litter_sample_dataset/paper/paper90.jpg'), PosixPath('litter_sample_dataset/paper/paper303.jpg'), PosixPath('litter_sample_dataset/paper/paper47.jpg'), PosixPath('litter_sample_dataset/paper/paper53.jpg'), PosixPath('litter_sample_dataset/paper/paper539.jpg'), PosixPath('litter_sample_dataset/paper/paper263.jpg'), PosixPath('litter_sample_dataset/paper/paper505.jpg'), PosixPath('litter_sample_dataset/paper/paper511.jpg'), PosixPath('litter_sample_dataset/paper/paper277.jpg'), PosixPath('litter_sample_dataset/paper/paper288.jpg'), PosixPath('litter_sample_dataset/paper/paper298.jpg'), PosixPath('litter_sample_dataset/paper/paper529.jpg'), PosixPath('litter_sample_dataset/paper/paper273.jpg'), PosixPath('litter_sample_dataset/paper/paper515.jpg'), PosixPath('litter_sample_dataset/paper/paper501.jpg'), PosixPath('litter_sample_dataset/paper/paper267.jpg'), PosixPath('litter_sample_dataset/paper/paper57.jpg'), PosixPath('litter_sample_dataset/paper/paper43.jpg'), PosixPath('litter_sample_dataset/paper/paper449.jpg'), PosixPath('litter_sample_dataset/paper/paper307.jpg'), PosixPath('litter_sample_dataset/paper/paper94.jpg'), PosixPath('litter_sample_dataset/paper/paper461.jpg'), PosixPath('litter_sample_dataset/paper/paper475.jpg'), PosixPath('litter_sample_dataset/paper/paper80.jpg'), PosixPath('litter_sample_dataset/paper/paper313.jpg'), PosixPath('litter_sample_dataset/paper/paper139.jpg'), PosixPath('litter_sample_dataset/paper/paper105.jpg'), PosixPath('litter_sample_dataset/paper/paper111.jpg'), PosixPath('litter_sample_dataset/paper/paper110.jpg'), PosixPath('litter_sample_dataset/paper/paper104.jpg'), PosixPath('litter_sample_dataset/paper/paper138.jpg'), PosixPath('litter_sample_dataset/paper/paper81.jpg'), PosixPath('litter_sample_dataset/paper/paper474.jpg'), PosixPath('litter_sample_dataset/paper/paper312.jpg'), PosixPath('litter_sample_dataset/paper/paper306.jpg'), PosixPath('litter_sample_dataset/paper/paper460.jpg'), PosixPath('litter_sample_dataset/paper/paper95.jpg'), PosixPath('litter_sample_dataset/paper/paper448.jpg'), PosixPath('litter_sample_dataset/paper/paper42.jpg'), PosixPath('litter_sample_dataset/paper/paper56.jpg'), PosixPath('litter_sample_dataset/paper/paper500.jpg'), PosixPath('litter_sample_dataset/paper/paper266.jpg'), PosixPath('litter_sample_dataset/paper/paper272.jpg'), PosixPath('litter_sample_dataset/paper/paper514.jpg'), PosixPath('litter_sample_dataset/paper/paper528.jpg'), PosixPath('litter_sample_dataset/paper/paper299.jpg'), PosixPath('litter_sample_dataset/paper/paper258.jpg'), PosixPath('litter_sample_dataset/paper/paper264.jpg'), PosixPath('litter_sample_dataset/paper/paper502.jpg'), PosixPath('litter_sample_dataset/paper/paper516.jpg'), PosixPath('litter_sample_dataset/paper/paper270.jpg'), PosixPath('litter_sample_dataset/paper/paper489.jpg'), PosixPath('litter_sample_dataset/paper/paper68.jpg'), PosixPath('litter_sample_dataset/paper/paper40.jpg'), PosixPath('litter_sample_dataset/paper/paper54.jpg'), PosixPath('litter_sample_dataset/paper/paper338.jpg'), PosixPath('litter_sample_dataset/paper/paper310.jpg'), PosixPath('litter_sample_dataset/paper/paper476.jpg'), PosixPath('litter_sample_dataset/paper/paper83.jpg'), PosixPath('litter_sample_dataset/paper/paper97.jpg'), PosixPath('litter_sample_dataset/paper/paper462.jpg'), PosixPath('litter_sample_dataset/paper/paper304.jpg'), PosixPath('litter_sample_dataset/paper/paper112.jpg'), PosixPath('litter_sample_dataset/paper/paper106.jpg'), PosixPath('litter_sample_dataset/paper/paper107.jpg'), PosixPath('litter_sample_dataset/paper/paper113.jpg'), PosixPath('litter_sample_dataset/paper/paper463.jpg'), PosixPath('litter_sample_dataset/paper/paper96.jpg'), PosixPath('litter_sample_dataset/paper/paper305.jpg'), PosixPath('litter_sample_dataset/paper/paper311.jpg'), PosixPath('litter_sample_dataset/paper/paper82.jpg'), PosixPath('litter_sample_dataset/paper/paper477.jpg'), PosixPath('litter_sample_dataset/paper/paper339.jpg'), PosixPath('litter_sample_dataset/paper/paper55.jpg'), PosixPath('litter_sample_dataset/paper/paper41.jpg'), PosixPath('litter_sample_dataset/paper/paper69.jpg'), PosixPath('litter_sample_dataset/paper/paper488.jpg'), PosixPath('litter_sample_dataset/paper/paper517.jpg'), PosixPath('litter_sample_dataset/paper/paper271.jpg'), PosixPath('litter_sample_dataset/paper/paper265.jpg'), PosixPath('litter_sample_dataset/paper/paper503.jpg'), PosixPath('litter_sample_dataset/paper/paper259.jpg')]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"litter_sample_dataset\"\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "paper_count = list(data_dir.glob('paper/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-53c167c94b5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  image_size=(256, 256),\n",
    "  seed=1,\n",
    "  batch_size=32)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=1,\n",
    "  image_size=(256, 256),\n",
    "  batch_size=32)\n",
    "\n",
    "for x, y in train_ds.take(5):\n",
    "  print('Image --> ', x.shape, 'Label --> ',  y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'augmentation', 'cardboard', 'glass', 'leaf', 'metal', 'paper', 'plastic', 'polythenebag', 'recyclable', 'wrapper']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b7d092e0ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(20):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 256, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eDn-bRD30sw"
   },
   "source": [
    "### Preprocess the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f80d2251830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f80d2251830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "0.0 0.9328432\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g0FqHC0yCg6n"
   },
   "outputs": [],
   "source": [
    "# # Pixel values in this dataset are between 0 and 255, and must be normalized to a value between 0 and 1 \n",
    "# # for processing by the model. Divide the values by 255 to make this adjustment.\n",
    "\n",
    "# train_images = (train_images / 255.0).astype(np.float32)\n",
    "# test_images = (test_images / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bbee849ab73"
   },
   "source": [
    "Convert the data labels to categorical values by performing one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fmc7EgYO30sw"
   },
   "outputs": [],
   "source": [
    "# train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "# test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79f5f372fb0e"
   },
   "source": [
    "Note: Make sure you preprocess your *training* and *testing* datasets in the same way, so that your testing accurately evaluate your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkuDUFNNyAVN"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "The following code runs model training for 100 epochs, processing batches of 100 images at a time, and displaying the loss value after every 10 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Diwn1MmkNVeX",
    "outputId": "c8dc596f-7442-48cf-cd76-7155c4916e97"
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 100\n",
    "# BATCH_SIZE = 100\n",
    "# epochs = np.arange(1, NUM_EPOCHS + 1, 1)\n",
    "# losses = np.zeros([NUM_EPOCHS])\n",
    "# m = Model()\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# for i in range(NUM_EPOCHS):\n",
    "#   for x,y in train_ds:\n",
    "#     result = m.train(x, y)\n",
    "\n",
    "#   losses[i] = result['loss']\n",
    "#   if (i + 1) % 10 == 0:\n",
    "#     print(f\"Finished {i+1} epochs\")\n",
    "#     print(f\"  loss: {losses[i]:.3f}\")\n",
    "\n",
    "# # Save the trained weights to a checkpoint.\n",
    "# m.save('/tmp/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Lp2nkZj7rJXm",
    "outputId": "3fa73713-40eb-4aa0-9fa8-6d490ec9815b"
   },
   "outputs": [],
   "source": [
    "# plt.plot(epochs, losses, label='Pre-training')\n",
    "# plt.ylim([0, max(plt.ylim())])\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss [Cross Entropy]')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f80edf188c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f80edf188c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.3252 - accuracy: 0.5061WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f80b829f3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f80b829f3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "92/92 [==============================] - 49s 531ms/step - loss: 1.3252 - accuracy: 0.5061 - val_loss: 1.0775 - val_accuracy: 0.5923\n",
      "Epoch 2/3\n",
      "92/92 [==============================] - 50s 548ms/step - loss: 0.9761 - accuracy: 0.6359 - val_loss: 0.9574 - val_accuracy: 0.6443\n",
      "Epoch 3/3\n",
      "92/92 [==============================] - 51s 558ms/step - loss: 0.7717 - accuracy: 0.7196 - val_loss: 0.8952 - val_accuracy: 0.6840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80d2252f50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaMMDLLewAaX"
   },
   "source": [
    "Note: You should complete initial training of your model before converting it to TensorFlow Lite format, so that the model has an initial set of weights, and is able to perform reasonable inferences *before* you start collecting data and conducting training runs on the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8YUTIvzMVw5"
   },
   "source": [
    "## Convert model to TensorFlow Lite format\n",
    "\n",
    "After you have extended your TensorFlow model to enable additional functions for on-device training and completed initial training of the model, you can convert it to TensorFlow Lite format. The following code converts and saves your model to that format, including the set of signatures that you use with the TensorFlow Lite model on a device: `train, infer, save, restore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "# START Arielle Convert to Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f80edb34320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f80edb34320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f80edb34320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f80edb34050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f80edb34050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f80edb34050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: /var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/tmpnm6f16iv/garbage_identifier/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/tmpnm6f16iv/garbage_identifier/1/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Save the model\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "garbage_identifier_save_path = os.path.join(tmpdir, \"garbage_identifier/1/\")\n",
    "tf.saved_model.save(model, garbage_identifier_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 10:08:31.601659: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-03-06 10:08:31.601672: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-03-06 10:08:31.601780: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/tmpnm6f16iv/garbage_identifier/1/\n",
      "2022-03-06 10:08:31.603623: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-03-06 10:08:31.603633: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/tmpnm6f16iv/garbage_identifier/1/\n",
      "2022-03-06 10:08:31.611247: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-03-06 10:08:31.671995: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/tmpnm6f16iv/garbage_identifier/1/\n",
      "2022-03-06 10:08:31.686925: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 85146 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(garbage_identifier_save_path) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and run a model in Python\n",
    "## Platform: Linux\n",
    "\n",
    "The Python API for running an inference is provided in the tf.lite module. From which, you mostly need only tf.lite.Interpreter to load a model and run an inference.\n",
    "\n",
    "The following example shows how to use the Python interpreter to load a .tflite file and run inference with random input data:\n",
    "\n",
    "**How to test the lite model when the model doesn't have SignatureDefs defined:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-26.740198   -3.0922892  15.715262   17.64233    22.267265   19.45554\n",
      "  -17.33106     7.6015053  -9.631777    7.1270623 -18.964811 ]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Arielle Convert to Lite Model\n",
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WwsDUEKFMYtq"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/vkd1cl5s6sz3crh_c71dj3x00000gn/T/ipykernel_3315/3545723740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m tf.saved_model.save(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mSAVED_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     signatures={\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL_DIR = \"saved_model\"\n",
    "\n",
    "tf.saved_model.save(\n",
    "    m,\n",
    "    SAVED_MODEL_DIR,\n",
    "    signatures={\n",
    "        'train':\n",
    "            m.train.get_concrete_function(),\n",
    "        'infer':\n",
    "            m.infer.get_concrete_function(),\n",
    "        'save':\n",
    "            m.save.get_concrete_function(),\n",
    "        'restore':\n",
    "            m.restore.get_concrete_function(),\n",
    "    })\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJSOtPISKSn2"
   },
   "source": [
    "### Setup the TensorFlow Lite signatures\n",
    "\n",
    "The TensorFlow Lite model you saved in the previous step contains several function signatures. You can access them through the `tf.lite.Interpreter` class and invoke each `restore`, `train`, `save`, and `infer` signature separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNX2vqXd2-HM"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "infer = interpreter.get_signature_runner(\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTOM4alkteTO"
   },
   "source": [
    "Compare the output of the original model, and the converted lite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDdaCmPEtE7P"
   },
   "outputs": [],
   "source": [
    "logits_original = m.infer(x=train_images[:1])['logits'][0]\n",
    "logits_lite = infer(x=train_images[:1])['logits'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpoZ1nTMKGEZ",
    "outputId": "4d3c212b-725e-4554-9997-858161d403ee"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def compare_logits(logits):\n",
    "  width = 0.35\n",
    "  offset = width/2\n",
    "  assert len(logits)==2\n",
    "\n",
    "  keys = list(logits.keys())\n",
    "  plt.bar(x = np.arange(len(logits[keys[0]]))-offset,\n",
    "      height=logits[keys[0]], width=0.35, label=keys[0])\n",
    "  plt.bar(x = np.arange(len(logits[keys[1]]))+offset,\n",
    "      height=logits[keys[1]], width=0.35, label=keys[1])\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.ylabel('Logit')\n",
    "  plt.xlabel('ClassID')\n",
    "\n",
    "  delta = np.sum(np.abs(logits[keys[0]] - logits[keys[1]]))\n",
    "  plt.title(f\"Total difference: {delta:.3g}\")\n",
    "\n",
    "compare_logits({'Original': logits_original, 'Lite': logits_lite})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARUb37Hqa0Az"
   },
   "source": [
    "Above, you can see that the behavior of the model is not changed by the conversion to TFLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5f73c8c5d6"
   },
   "source": [
    "## Retrain the model on a device\n",
    "\n",
    "After converting your model to TensorFlow Lite and deploying it with your app, you can retrain the model on a device using new data and the `train` signature method of your model. Each training run generates a new set of weights that you can save for re-use and further improvement of the model, as shown in the next section.\n",
    "\n",
    "Note: Since training tasks are resource intensive, you should consider performing them when users are not actively interacting with the device, and as a background process. Consider using the [WorkManager](https://developer.android.com/topic/libraries/architecture/workmanager) API to schedule model retraining as an asynchronous task.\n",
    "\n",
    "On Android, you can perform on-device training with TensorFlow Lite using either Java or C++ APIs. In Java, use the `Interpreter` class to load a model and drive model training tasks. The following example shows how to run the training procedure using the `runSignature` method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvbqRxnNs4NG"
   },
   "source": [
    "```Java\n",
    "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
    "    int NUM_EPOCHS = 100;\n",
    "    int BATCH_SIZE = 100;\n",
    "    int IMG_HEIGHT = 28;\n",
    "    int IMG_WIDTH = 28;\n",
    "    int NUM_TRAININGS = 60000;\n",
    "    int NUM_BATCHES = NUM_TRAININGS / BATCH_SIZE;\n",
    "\n",
    "    List<FloatBuffer> trainImageBatches = new ArrayList<>(NUM_BATCHES);\n",
    "    List<FloatBuffer> trainLabelBatches = new ArrayList<>(NUM_BATCHES);\n",
    "\n",
    "    // Prepare training batches.\n",
    "    for (int i = 0; i < NUM_BATCHES; ++i) {\n",
    "        FloatBuffer trainImages = FloatBuffer.allocateDirect(BATCH_SIZE * IMG_HEIGHT * IMG_WIDTH).order(ByteOrder.nativeOrder());\n",
    "        FloatBuffer trainLabels = FloatBuffer.allocateDirect(BATCH_SIZE * 10).order(ByteOrder.nativeOrder());\n",
    "\n",
    "        // Fill the data values...\n",
    "        trainImageBatches.add(trainImages.rewind());\n",
    "        trainImageLabels.add(trainLabels.rewind());\n",
    "    }\n",
    "\n",
    "    // Run training for a few steps.\n",
    "    float[] losses = new float[NUM_EPOCHS];\n",
    "    for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {\n",
    "        for (int batchIdx = 0; batchIdx < NUM_BATCHES; ++batchIdx) {\n",
    "            Map<String, Object> inputs = new HashMap<>();\n",
    "            inputs.put(\"x\", trainImageBatches.get(batchIdx));\n",
    "            inputs.put(\"y\", trainLabelBatches.get(batchIdx));\n",
    "\n",
    "            Map<String, Object> outputs = new HashMap<>();\n",
    "            FloatBuffer loss = FloatBuffer.allocate(1);\n",
    "            outputs.put(\"loss\", loss);\n",
    "\n",
    "            interpreter.runSignature(inputs, outputs, \"train\");\n",
    "\n",
    "            // Record the last loss.\n",
    "            if (batchIdx == NUM_BATCHES - 1) losses[epoch] = loss.get(0);\n",
    "        }\n",
    "\n",
    "        // Print the loss output for every 10 epochs.\n",
    "        if ((epoch + 1) % 10 == 0) {\n",
    "            System.out.println(\n",
    "              \"Finished \" + (epoch + 1) + \" epochs, current loss: \" + loss.get(0));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // ...\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPEzyHAZs7Gl"
   },
   "source": [
    "You can see a complete code example of model retraining inside an Android app in the [model personalization demo app](https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/android/transfer_api/src/main/java/org/tensorflow/lite/examples/transfer/api/LiteMultipleSignatureModel.java)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuBlL9z5GASQ"
   },
   "source": [
    "Run training for a few epochs to improve or personalize the model. In practice, you would run this additional training using data collected on the device. For simplicity, this example uses the same training data as the previous training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjQ5xrhyGcIQ",
    "outputId": "e6b9ff39-ffde-4f21-9ceb-358165799b90"
   },
   "outputs": [],
   "source": [
    "train = interpreter.get_signature_runner(\"train\")\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "more_epochs = np.arange(epochs[-1]+1, epochs[-1] + NUM_EPOCHS + 1, 1)\n",
    "more_losses = np.zeros([NUM_EPOCHS])\n",
    "\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "  for x,y in train_ds:\n",
    "    result = train(x=x, y=y)\n",
    "  more_losses[i] = result['loss']\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print(f\"Finished {i+1} epochs\")\n",
    "    print(f\"  loss: {more_losses[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX7dQXx_iPuv",
    "outputId": "747ecb65-c404-4284-b4d2-d4c9c80a122a"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, losses, label='Pre-training')\n",
    "plt.plot(more_epochs, more_losses, label='On device')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [Cross Entropy]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbe9_LjAbEMF"
   },
   "source": [
    "Above you can see that the on-device training picks up exactly where the pretraining stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDIi0_RlPb2n"
   },
   "source": [
    "## Save the trained weights\n",
    "\n",
    "When you complete a training run on a device, the model updates the set of weights it is using in memory. Using the `save` signature method you created in your TensorFlow Lite model, you can save these weights to a checkpoint file for later reuse and improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c3d3cc5f171",
    "outputId": "36d0a964-f1b3-41c8-9504-61f6abf2d7c4"
   },
   "outputs": [],
   "source": [
    "save = interpreter.get_signature_runner(\"save\")\n",
    "\n",
    "save(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvlZN-rhR_Ag"
   },
   "source": [
    "In your Android application, you can store the generated weights as a checkpoint file in the internal storage space allocated for your app.\n",
    "\n",
    "```Java\n",
    "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
    "    // Conduct the training jobs.\n",
    "\n",
    "    // Export the trained weights as a checkpoint file.\n",
    "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
    "    Map<String, Object> inputs = new HashMap<>();\n",
    "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
    "    Map<String, Object> outputs = new HashMap<>();\n",
    "    interpreter.runSignature(inputs, outputs, \"save\");\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSDydMyOQfL5"
   },
   "source": [
    "## Restore the trained weights\n",
    "\n",
    "Any time you create an interpreter from a TFLite model, the interpreter will initially load the original model weights.\n",
    "\n",
    "So after you've done some training and saved a checkpoint file, you'll need to run the `restore` signature method to load the checkpoint.\n",
    "\n",
    "A good rule is \"Anytime you create an Interpreter for a model, if the checkpoint exists, load it\". If you need to reset the model to the baseline behavior, just delete the checkpoint and create a fresh interpreter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yIZoLveRZgp"
   },
   "outputs": [],
   "source": [
    "another_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "another_interpreter.allocate_tensors()\n",
    "\n",
    "infer = another_interpreter.get_signature_runner(\"infer\")\n",
    "restore = another_interpreter.get_signature_runner(\"restore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjiUbx7zIoLq",
    "outputId": "6d77b806-58e2-4ad4-9cb0-42685a2241a8"
   },
   "outputs": [],
   "source": [
    "logits_before = infer(x=train_images[:1])['logits'][0]\n",
    "\n",
    "# Restore the trained weights from /tmp/model.ckpt\n",
    "restore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))\n",
    "\n",
    "logits_after = infer(x=train_images[:1])['logits'][0]\n",
    "\n",
    "compare_logits({'Before': logits_before, 'After': logits_after})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7T6pja2bPqV"
   },
   "source": [
    "The checkpoint was generated by training and saving with TFLite. Above you can see that applying the checkpoint updates the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAo-3Eg7oGH7"
   },
   "source": [
    "Note: Loading the saved weights from the checkpoint can take time, based on the number of variables in the model and the size of the checkpoint file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9I_-gjdSnGn"
   },
   "source": [
    "In your Android app, you can restore the serialized, trained weights from the checkpoint file you stored earlier.\n",
    "\n",
    "```Java\n",
    "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
    "    // Load the trained weights from the checkpoint file.\n",
    "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
    "    Map<String, Object> inputs = new HashMap<>();\n",
    "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
    "    Map<String, Object> outputs = new HashMap<>();\n",
    "    anotherInterpreter.runSignature(inputs, outputs, \"restore\");\n",
    "}\n",
    "```\n",
    "\n",
    "Note: When your application restarts, you should reload your trained weights prior to running new inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjcrv57DSkz2"
   },
   "source": [
    "## Run Inference using trained weights\n",
    "\n",
    "Once you have loaded previously saved weights from a checkpoint file, running the `infer` method uses those weights with your original model to improve predictions. After loading the saved weights, you can use the `infer` signature method as shown below.\n",
    "\n",
    "Note: Loading the saved weights is not required to run an inference, but running in that configuration produces predictions using the originally trained model, without improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ROmlpHWS0nX"
   },
   "outputs": [],
   "source": [
    "infer = another_interpreter.get_signature_runner(\"infer\")\n",
    "result = infer(x=test_images)\n",
    "predictions = np.argmax(result[\"output\"], axis=1)\n",
    "\n",
    "true_labels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6nHopKlAD6-",
    "outputId": "97a2b8c3-7307-4df1-dede-e9694e021092"
   },
   "outputs": [],
   "source": [
    "result['output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGPtqeVULZui"
   },
   "source": [
    "Plot the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHbRasdfasd4",
    "outputId": "93cb0443-961a-4a73-dac4-c6472d0d6e5d"
   },
   "outputs": [],
   "source": [
    "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# def plot(images, predictions, true_labels):\n",
    "#   plt.figure(figsize=(10,10))\n",
    "#   for i in range(25):\n",
    "#       plt.subplot(5,5,i+1)\n",
    "#       plt.xticks([])\n",
    "#       plt.yticks([])\n",
    "#       plt.grid(False)\n",
    "#       plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "#       color = 'b' if predictions[i] == true_labels[i] else 'r'\n",
    "#       plt.xlabel(class_names[predictions[i]], color=color)\n",
    "#   plt.show()\n",
    "\n",
    "# plot(test_images, predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm8BxJOm_4rG",
    "outputId": "063c1b31-617b-4c57-cb6e-36df80d4abf2"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eijDL3jNS6WI"
   },
   "source": [
    "In your Android application, after restoring the trained weights, run the inferences based on the loaded data.\n",
    "\n",
    "```Java\n",
    "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
    "    // Restore the weights from the checkpoint file.\n",
    "\n",
    "    int NUM_TESTS = 10;\n",
    "    FloatBuffer testImages = FloatBuffer.allocateDirect(NUM_TESTS * 28 * 28).order(ByteOrder.nativeOrder());\n",
    "    FloatBuffer output = FloatBuffer.allocateDirect(NUM_TESTS * 10).order(ByteOrder.nativeOrder());\n",
    "\n",
    "    // Fill the test data.\n",
    "\n",
    "    // Run the inference.\n",
    "    Map<String, Object> inputs = new HashMap<>();\n",
    "    inputs.put(\"x\", testImages.rewind());\n",
    "    Map<String, Object> outputs = new HashMap<>();\n",
    "    outputs.put(\"output\", output);\n",
    "    anotherInterpreter.runSignature(inputs, outputs, \"infer\");\n",
    "    output.rewind();\n",
    "\n",
    "    // Process the result to get the final category values.\n",
    "    int[] testLabels = new int[NUM_TESTS];\n",
    "    for (int i = 0; i < NUM_TESTS; ++i) {\n",
    "        int index = 0;\n",
    "        for (int j = 1; j < 10; ++j) {\n",
    "            if (output.get(i * 10 + index) < output.get(i * 10 + j)) index = testLabels[j];\n",
    "        }\n",
    "        testLabels[i] = index;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "garbage_objectIdentification_ramudroid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
